---
title: DBWritable与Writable的坑
date: 2020-1-18 21:20:14
tags:
 - hadoop
 - mysql
categories: 大数据
---

export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:~/lib/mysql-connector-java-5.1.28-bin.jar



## 本地运行，从mysql读取数据跑mapreduce

**接口实现不仅仅要实现DBWritable，还应当实现Writable！！！****



DeptWritable.java

```
import org.apache.hadoop.io.Writable;
import org.apache.hadoop.mapreduce.lib.db.DBWritable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;

/**
 * author：若泽数据-PK哥
 * 交流群：545916944
 */
public class DeptWritable implements Writable, DBWritable {
    private int deptid;
    private int deptno;
    private String dname;
    private String loc;

    public DeptWritable(){}

    public DeptWritable(int deptid, int deptno,String dname,String loc){
        this.deptid = deptid;
        this.deptno = deptno;
        this.dname = dname;
        this.loc = loc;
    }

    public int getDeptid() {
        return deptid;
    }

    public void setDeptid(int deptid) {
        this.deptid = deptid;
    }

    public int getDeptno() {
        return deptno;
    }

    public void setDeptno(int deptno) {
        this.deptno = deptno;
    }

    public String getDname() {
        return dname;
    }

    public void setDname(String dname) {
        this.dname = dname;
    }

    public String getLoc() {
        return loc;
    }

    public void setLoc(String loc) {
        this.loc = loc;
    }

    @Override
    public void write(PreparedStatement statement) throws SQLException {
        statement.setInt(1,deptid);
        statement.setInt(2,deptno);
        statement.setString(3,dname);
        statement.setString(4,loc);
    }

    @Override
    public void readFields(ResultSet resultSet) throws SQLException {
        deptid = resultSet.getInt(1);
        deptno = resultSet.getInt(2);
        dname = resultSet.getString(3);
        loc = resultSet.getString(4);
    }

    @Override
    public void write(DataOutput out) throws IOException {
        out.writeInt(deptid);
        out.writeInt(deptno);
        out.writeUTF(dname);
        out.writeUTF(loc);
    }

    @Override
    public void readFields(DataInput in) throws IOException {
        this.deptid = in.readInt();
        this.deptno = in.readInt();
        this.dname = in.readUTF();
        this.loc = in.readUTF();
    }

    @Override
    public String toString() {
        return deptid + "\t"
                + "\t" + deptno
                + "\t" + dname
                + "\t" + loc;
    }
}
```



MySQLReadDriver.java（该方案可以在本地运行，但打成瘦包后在服务器上不能运行）

```java
import com.ruozedata.bigdata.hadoop.utils.FileUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.db.DBConfiguration;
import org.apache.hadoop.mapreduce.lib.db.DBInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class MySQLReadDriver {

    public static void main(String[] args) throws Exception {
        String output = "out";

        // 1）获取Job对象
        Configuration configuration = new Configuration();
//        configuration.set(DBConfiguration.DRIVER_CLASS_PROPERTY, "com.mysql.jdbc.Driver");
        DBConfiguration.configureDB(configuration, "com.mysql.jdbc.Driver", "jdbc:mysql://hadoop000:3306/sqoop", "hadoop", "hadoop");

        Job job = Job.getInstance(configuration);
        FileUtils.deleteOutput(configuration, output);

        // 2）本job对应要执行的主类是哪个
        job.setJarByClass(MySQLReadDriver.class);

        // 3）设置Mapper和Reducer
        job.setMapperClass(MyMapper.class);

        // 4）设置Mapper阶段输出数据的类型
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(DeptWritable.class);

        // 6）设置输入和输出路径
        String[] fields = {"id", "deptno", "dname", "loc"};
        DBInputFormat.setInput(job, DeptWritable.class, "dept", null, null, fields);

        FileOutputFormat.setOutputPath(job, new Path(output));

        // 7）提交作业
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);

    }

    public static class MyMapper extends Mapper<LongWritable, DeptWritable, NullWritable, DeptWritable> {

        @Override
        protected void map(LongWritable key, DeptWritable value, Context context) throws IOException, InterruptedException {
            context.write(NullWritable.get(), value);
        }
    }
}
```



MySQLReadDriverv2.java（该方案打成瘦包后可以在服务器上运行）

**extends Configured implements Tool**

```java
import com.ruozedata.bigdata.hadoop.utils.FileUtils;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.db.DBConfiguration;
import org.apache.hadoop.mapreduce.lib.db.DBInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

import java.io.IOException;

public class MySQLReadDriverV2 extends Configured implements Tool{

    public static void main(String[] args) throws Exception {
        Configuration configuration = new Configuration();
        int run = ToolRunner.run(configuration, new MySQLReadDriverV2(), args);
        System.exit(run);
    }

    @Override
    public int run(String[] args) throws Exception {

        String output = "out";

        // 1）获取Job对象
        Configuration configuration = super.getConf();
//        configuration.set(DBConfiguration.DRIVER_CLASS_PROPERTY, "com.mysql.jdbc.Driver");
        DBConfiguration.configureDB(configuration, "com.mysql.jdbc.Driver", "jdbc:mysql://hadoop000:3306/sqoop", "hadoop", "4WOzuishuai");

        Job job = Job.getInstance(configuration);
        FileUtils.deleteOutput(configuration, output);

        // 2）本job对应要执行的主类是哪个
        job.setJarByClass(MySQLReadDriverV2.class);

        // 3）设置Mapper和Reducer
        job.setMapperClass(MyMapper.class);

        // 4）设置Mapper阶段输出数据的类型
        job.setMapOutputKeyClass(NullWritable.class);
        job.setMapOutputValueClass(DeptWritable.class);

        // 6）设置输入和输出路径
        String[] fields = {"id", "deptno", "dname", "loc"};
        DBInputFormat.setInput(job, DeptWritable.class, "dept", null, null, fields);

        FileOutputFormat.setOutputPath(job, new Path(output));

        // 7）提交作业
        boolean result = job.waitForCompletion(true);
        return 1;
    }

    public static class MyMapper extends Mapper<LongWritable, DeptWritable, NullWritable, DeptWritable> {

        @Override
        protected void map(LongWritable key, DeptWritable value, Context context) throws IOException, InterruptedException {
            context.write(NullWritable.get(), value);
        }
    }

}
```

