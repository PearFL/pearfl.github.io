---
title: 浅谈flume的基础使用
date: 2020-2-13 15:22:22
tags:
 - flume
categories: 大数据
---



**flume官方文档：**http://flume.apache.org/



![41_1](/images/41/41_1.png)

从系统架构图上来看，用户只要产生行为，那么日志就会在Nginx服务器中保存，所以我们现在要做的就是把数据从Nginx服务器中使用Flume采集到HDFS上

<!-- more -->

## flume怎么用？

**flume**

Flume就是一个针对日志数据进行采集和汇总的一个框架

Flume的进程叫做Agent，每个Agent中有Srouce、Channel？Sink

Flume从使用层面来讲就是写配置文件，其实就是配置我们的Agent，只要学会从官网查配置就行了



**Agent**

Source中的常用方式有 avro、exec、spooling、taildir、kafka

Channel中的常用方式有 memory、kafka、file

Sink中的常用方式有 hdfs、logger、avro、kafka



## flume编译安装和配置

先配置好maven环境（参考之前的博文），解压flume-ng-1.6.0-cdh5.16.2-src.tar.gz并进入解压路径

```
# tar -zxvf flume-ng-1.6.0-cdh5.16.2-src.tar.gz
```

编译：

```
mvn clean compile validate -Denforcer.skip=true
```

提示：flume-ng-morphline-solr-sink我们用不到，可以直接注释掉，在flume-ng-sinks下的pom中找到并注释

```
 <modules>
    <module>flume-hdfs-sink</module>
    <module>flume-irc-sink</module>
    <module>flume-ng-hbase-sink</module>
    <module>flume-ng-elasticsearch-sink</module>
    <!--<module>flume-ng-morphline-solr-sink</module> -->
    <module>flume-ng-kafka-sink</module>
  </modules>
```

在.bash_profile里配置

```
export FLUME_HOME=/home/pearfl/app/apache-flume-1.6.0-cdh5.16.2-bin
export PATH=$FLUME_HOME/bin:$PATH
```

conf目录下的flume-env.sh需要配置jdk路径

```
export JAVA_HOME=/home/pearfl/app/jdk1.8.0_231
```



<!--more-->

## flume核心组件

```
Source	从哪收集
	avro	序列化
	exec	命令行
	spooling	目录
	taildir	******
	kafka

Channel	数据存哪里
	memory
	kafka
	file
	
Sink	数据输出到哪里
	hdfs	Hadoop
	logger	控制台
	avro	
	kafka
```



## flume简单配置文件

```nginx
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

解释说明

```
example.conf	<=	flume配置文件的文件名
a1	<=	agent的名字
r1	<=	source的名字
k1	<=	sink的名字
c1	<=	channel的名字
```

我们在flume文件夹下，进去后新建script文件夹，在script文件夹内创建example.conf

```
# mkdir script
# cd script/
# vi example.conf

a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = netcat
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 44444

a1.sinks.k1.type = logger

a1.channels.c1.type = memory

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

启动命令

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/example.conf -Dflume.root.logger=INFO,console
```



```
Event:	Flume数据传输的基本单元
event	==>	flume	==>	dst
	header	可选的
		k-v
	body	byte	array
```



## flume进一步理解


追踪日志不断增加的内容，并且输出到hdfs中（单文件）
```nginx
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/pearfl/data/flume/data.log
a1.sources.r1.shell = /bin/sh -c

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop000:8020/ruozedata/flume/tail
a1.sinks.k1.batchSize = 10
a1.sinks.k1.fileType = DataStream
a1.sinks.k1.writeFormat = Text

a1.channels.c1.type = memory

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/flume-exec-hdfs.conf -Dflume.root.logger=INFO,console
```



目录

```nginx
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/pearfl/data/flume/spool/

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop000:8020/ruozedata/flume/spool/%Y%m%d%H%M
a1.sinks.k1.batchSize = 10
a1.sinks.k1.fileType = DataStream
a1.sinks.k1.writeFormat = Text
a1.sinks.k1.filePrefix = bigdata-
a1.sinks.k1.useLocalTimeStamp = true

a1.channels.c1.type = memory

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/flume-spool-hdfs.conf -Dflume.root.logger=INFO,console
```



## 生产上常用tairdir

文档：http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source

TAIRDIR
	offset
	不仅仅可以支持到file，也可以支持的directory

```nginx
a1.sources = r1
a1.sinks = k1
a1.channels = c1

a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1 f2
a1.sources.r1.positionFile = /home/pearfl/data/flume/taildir_position.json
a1.sources.r1.filegroups.f1 = /home/pearfl/data/flume/taildir/test1/example.log
a1.sources.r1.headers.f1.headerKey1 = value1
a1.sources.r1.filegroups.f2 = /home/pearfl/data/flume/taildir/test2/.*log.*
a1.sources.r1.headers.f2.headerKey1 = value2
a1.sources.r1.headers.f2.headerKey2 = value2-2

a1.channels.c1.type = memory

a1.sinks.k1.type = logger

a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/flume-taildir-logger.conf -Dflume.root.logger=INFO,console
```

其中`taildir_position.json`文件是用来记录文件读取offset的位置，方便下次继续从offset位置读取

注意`taildir_position.json`文件不能存在上级目录，不然会报错



## Flume远程实时采集Windows产生的log4j产生的数据（控制台输出）

IDEA上的pom.xml添加

```
<!-- flume采集 -->
    <dependency>
      <groupId>org.apache.flume.flume-ng-clients</groupId>
      <artifactId>flume-ng-log4jappender</artifactId>
      <version>1.6.0</version>
    </dependency>
```

IDEA上的log4j.properties

```
log4j.rootLogger=INFO,stdout,flume
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n

log4j.appender.flume = org.apache.flume.clients.log4jappender.Log4jAppender
log4j.appender.flume.Hostname = hadoop000
log4j.appender.flume.Port = 41414
log4j.appender.flume.UnsafeMode = true
log4j.appender.flume.layout=org.apache.log4j.PatternLayout
log4j.appender.flume.layout.ConversionPattern=%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] [%c] [%p] - %m%n
```



streaming.conf

```nginx
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# 配置监控文件
a1.sources.r1.type = avro
a1.sources.r1.bind=0.0.0.0
a1.sources.r1.port=41414

# 配置sink
a1.sinks.k1.type =logger

# a1.sinks.k1.type = file_roll
# a1.sinks.k1.sink.directory =/home/pearfl/tmp/stream
# a1.sinks.k1.sink.rollInterval=300
# a1.sinks.k1.sink.batchSize=100
# a1.sinks.k1.sink.serializer=TEXT
# a1.sinks.k1.sink.serializer.appendNewline = false

#-------------
# 配置channel
a1.channels.c1.type = memory
# 将三者串联
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/streaming.conf -Dflume.root.logger=INFO,console
```



## Flume远程实时采集Windows产生的log4j产生的数据（控制台输出存入hdfs）

```nginx
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# 配置监控文件
a1.sources.r1.type = avro
a1.sources.r1.bind=0.0.0.0
a1.sources.r1.port=41414

# 配置sink
# a1.sinks.k1.type =logger
#a1.sinks.k1.maxBytesToLog = 50

a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://hadoop000:8020/data/tmp/%Y%m%d%H%M
a1.sinks.k1.hdfs.batchSize = 10
a1.sinks.k1.hdfs.fileType = DataStream
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.useLocalTimeStamp = true

#-------------
# 配置channel
a1.channels.c1.type = memory
# 将三者串联
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```

```
# flume-ng agent --name a1 --conf $FLUME_HOME/conf --conf-file $FLUME_HOME/script/hdfs-stream.conf -Dflume.root.logger=INFO,console
```



## 解决Flume采集数据时生成大量小文件的问题

在使用Flume采集数据时，由于默认参数的影响会生产大量小文件，我们先看默认参数

```
hdfs.rollInterval	30	滚动当前文件之前要等待的秒数
hdfs.rollSize	1024	触发滚动当前文件的大小，单位bytes(B)
hdfs.rollCount	10		触发滚动当前文件的events数量
```

我们看到默认生成文件有三个条件，每30秒、每1M、每10个events，这样的配置会生成大量的小文件，所以我们要对这三个文件进行修改

最终生成的文件必须综合时间、文件大小、event数量来决定，时间太长或者文件太大都不利于最终生成的文件。该时间还需要配合`hdfs.path`参数指定的生成文件时间。

注意`sink.type`如果是`memory`模式，注意文件的大小，防止内存不足，太大可以设置`sink.type = file`