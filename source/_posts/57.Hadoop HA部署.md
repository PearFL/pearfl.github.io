---
title: Hadoop HA部署
date: 2020-3-17 14:28:44
tags:
 - hadoop
 - HA
categories: 大数据

---

# 集群搭建

## 服务器创建/购买

本博文采用阿里云ECS云服务器创建实例

当然，虚拟机创建3个实例也是没有问题的

3台服务器的配置均为2核8G，40G磁盘

<!-- more -->

## 安装阶段

创建文件夹，下载依赖，上传安装包
```
yum install -y lrzsz
useradd hadoop
su - hadoop
mkdir app software log data tmp

回到root用户
rz
(上传)通过rz上传hadoop,jdk和zookeeper安装包,再通过scp经过内网传给另外2台机器
scp * 172.16.135.139:/home/hadoop/software
scp * 172.16.135.140:/home/hadoop/software

```

配置hosts文件

```
172.16.135.140	ruozedata001  
172.16.135.139	ruozedata002
172.16.135.138	ruozedata003

echo "172.16.135.140   ruozedata001" >> /etc/hosts
echo "172.16.135.139   ruozedata002" >> /etc/hosts
echo "172.16.135.138   ruozedata003" >> /etc/hosts
```



配置ssh互相信任关系

```
(切换到hadoop用户下)
# 三台机器都在hadoop下执行ssh-keygen，之后一直选择回车
# 进入hadoop家目录下的.ssh
ssh-keygen
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
# 拷贝~/.ssh/authorized_keys到一起
cat ~/.ssh/authorized_keys
# 3个ssh-rsa弄一起
```



验证互相信任关系(三台机器都要做)

```
ssh ruozedata001 date
ssh ruozedata002 date
ssh ruozedata003 date
```



jdk安装

```
# jdk
# 目录必须: /usr/java/
mkdir /usr/java
mv jdk-8u45-linux-x64.gz /usr/java/
cd /usr/java
tar -zxvf jdk-8u45-linux-x64.gz
chown -R root:root /usr/java/*

# 环境变量
vi /etc/profile

export JAVA_HOME=/usr/java/jdk1.8.0_45
export PATH=${JAVA_HOME}/bin:$PATH

source /etc/profile
```



zookeeper安装

```
tar -zxvf zookeeper-3.4.5-cdh5.16.2.tar.gz -C ../app/
ln -s zookeeper-3.4.5-cdh5.16.2 zookeeper

cd zookeeper/conf
cp zoo_sample.cfg zoo.cfg
vi zoo.cfg
----下面是zoo.cfg内修改的数据
dataDir=/home/hadoop/data/zookeeper

server.1=ruozedata001:2888:3888
server.2=ruozedata002:2888:3888
server.3=ruozedata003:2888:3888
----

mkdir -p /home/hadoop/data/zookeeper
cd /home/hadoop/data/zookeeper

# 分别把1、2、3配置到三台机器里
echo 1 > myid
echo 2 > myid
echo 3 > myid

export ZOOKEEPER_HOME=/home/hadoop/app/zookeeper
export PATH=$ZOOKEEPER_HOME/bin:$PATH


```



hadoop安装

```
tar -zxvf hadoop-2.6.0-cdh5.16.2.tar.gz -C ../app/
ls -s hadoop-2.6.0-cdh5.16.2 hadoop

export HADOOP_HOME=/home/hadoop/app/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
```



## 配置阶段

core-site.xml配置介绍

```
fs.trash.interval #回收站机制
fs.trash.checkpoint.interval #垃圾回收站的检查间隔
```

**windows上传文件的格式会有坑！**

```
windows上传到linux会有格式问题，隐藏了window的换行等编码
yum install -y dos2unix
file slaves
转换格式
dos2unix slaves
```



hadoop-env.sh

```
export JAVA_HOME=/usr/java/jdk1.8.0_45
```



启动hadoop（hdfs+yarn）

```
# 格式化前先启动journalnode
./hadoop-daemon.sh start journalnode
# 进行格式化
# 选择第一台机器
bin/hadoop namenode -format

# 格式化完成后
# 同步到第二台HA
scp -r /home/hadoop/data/dfs/name ruozedata002:/home/hadoop/data/dfs

# 初始化ZKFC（在第一个节点上做）
bin/hdfs zkfc -formatZK

# 之后第一个节点上启动
sbin/start-dfs.sh

# 如果某个进程挂了，单独启动一个进程
hadoop-daemon.sh start namenode
hadoop-daemon.sh start datanode
hadoop-daemon.sh start journalnode
hadoop-daemon.sh start zkfc

# 第一台机器启动Yarn框架
./start-yarn.sh

# 第二台机器需要手动启动
yarn-daemon.sh start resourcemanager

# 启动jobhistory（端口在19888）
mr-jobhistory-daemon.sh start historyserver
```



查看web界面，需要在阿里云上第一台机器配置安全组允许任意机器访问



```
8080的yarn资源页面，主节点可以打开，从节点需要在后面加上/cluster/cluster才行
```





## 补充知识点

```
从节点（Standby节点）任何事情都不能干

手动杀掉active的namenode，从节点变为主节点
之后通过hadoop-daemon.sh start namenode重新启动

脑裂的概念：同时出现两个active或者两个standby
Haadmin

hdfs haadmin -getServiceState nn1 查看service状态（active  or  standby）
hdfs haadmin -checkHealth nn1   查看健康

hdfs haadmin  -transitionToActive  nn1 --forceactive  强制变为Active引发脑裂，不过系统不给你执行

hdfs haadmin -failover nn2 nn1   nn2的active转给nn1


hdfs getconf -confKey   可以获得配置文件的value值  
例如：
hdfs getconf -confKey dfs.nameservices   
ruozeclusterg8
hdfs getconf -confKey dfs.ha.namenodes.ruozeclusterg8
nn1,nn2
hdfs haadmin -getServiceState nn1
hdfs getconf -confKey dfs.namenode.rpc-address.ruozeclusterg8.nn1
ruozedata001:8020
```



