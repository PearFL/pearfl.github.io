---
title: 数据倾斜解决方案
date: 2020-2-4 19:34:15
tags:
 - hadoop
categories: 大数据
---



同key分发到同一个reduce去执行==>skew

数据倾斜解决方案：打散再处理

假设原task要处理9000w的数据量，我们通过随机数打散为100份

==> 原来都在一个reducetask中取处理的数据，被我们打散后变为9000w / 100

```mr
chain:
	第一个mr：key加随机数打散
	data_8 100
	data_1 200
	data_2 300
	data_4 200
	第二个mr：第一个mr加的随机数去掉
	data 100
	data 200
	data 300
	data 200
	
	在第二个mr中的reduce来做最终的聚合操作
	ruoze 800
```

Hadoop不怕数据量大，就怕数据倾斜hhhhhhhhhhhhh（跑不动）



以统计某人的数字之和为例

数据

```
mxh 	102
mxh 	1042
pearfl 	102222
pearfl 	1000
```



第一个mr

```java
public class ChainFirstDriver {
    public static void main(String[] args) throws Exception{
        String input = "data/chain/data.txt";
        String output = "data/chain/first";

        // 1:获取job对象
        Configuration configuration = new Configuration();
        Job job = Job.getInstance(configuration);

        //删除已存在目录
        FileUtils.deleteOutput(configuration,output);

        // 2：本job对应执行的主类
        job.setJarByClass(ChainFirstDriver.class);

        // 3）设置Mapper和Reducer
        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReducer.class);

        job.setNumReduceTasks(10);

        // 4）设置Mapper阶段输出数据的类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // 5）设置Reducer阶段输出数据的类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 6）设置输入和输出路径
        FileInputFormat.setInputPaths(job, new Path(input));
        FileOutputFormat.setOutputPath(job, new Path(output));

        // 7）提交作业
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }

    public static class MyMapper extends Mapper<LongWritable,Text,Text,IntWritable>{
        Random random = new Random();

        protected void map(LongWritable key,Text values,Context context) throws IOException, InterruptedException {
            String[] splits = values.toString().split("\t");
            int r = random.nextInt(10) + 1;
            context.write(new Text(splits[0].trim() + "_" + r), new IntWritable(Integer.parseInt(splits[1])));
        }
    }

    public static class MyReducer extends Reducer<Text, IntWritable,Text,IntWritable>{

        protected void reduce(Text key,Iterable<IntWritable>values,Context context) throws IOException, InterruptedException {
            int sum = 0;
            for(IntWritable value:values){
                sum+=value.get();
            }
            context.write(key,new IntWritable(sum));
        }
    }
}
```



第二个mr

```java
public class ChainSecondDriver {
    public static void main(String[] args) throws  Exception{
        String input = "data/chain/first";
        String output = "data/chain/second";

        // 1:获取job对象
        Configuration configuration = new Configuration();
        Job job = Job.getInstance(configuration);

        //删除已存在目录
        FileUtils.deleteOutput(configuration,output);

        // 2：本job对应执行的主类
        job.setJarByClass(ChainFirstDriver.class);

        // 3）设置Mapper和Reducer
        job.setMapperClass(MyMapper.class);
        job.setReducerClass(MyReducer.class);

        // 4）设置Mapper阶段输出数据的类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(IntWritable.class);

        // 5）设置Reducer阶段输出数据的类型
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);

        // 6）设置输入和输出路径
        FileInputFormat.setInputPaths(job, new Path(input));
        FileOutputFormat.setOutputPath(job, new Path(output));

        // 7）提交作业
        boolean result = job.waitForCompletion(true);
        System.exit(result ? 0 : 1);
    }

    public static class MyMapper extends Mapper<LongWritable,Text,Text,IntWritable>{

        protected void map(LongWritable key,Text values,Context context) throws IOException, InterruptedException {
            String[] splits = values.toString().split("\t");
            int index = splits[0].lastIndexOf("_");
            String result = splits[0].substring(0, index);
            context.write(new Text(result),new IntWritable(Integer.parseInt(splits[1])));
        }
    }

    public static class MyReducer extends Reducer<Text,IntWritable,Text,IntWritable>{

        protected void reduce(Text key,Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
            int sum=0;
            for(IntWritable value:values){
                sum+=value.get();
            }
            context.write(key,new IntWritable(sum));
        }
    }
}
```



FileUtils补充

```java
public class FileUtils {
    public static void deleteOutput(Configuration configuration, String output) throws Exception{
        FileSystem fileSystem = FileSystem.get(configuration);
        Path path = new Path(output);
        if(fileSystem.exists(path)){
            fileSystem.delete(path, true);
        }
    }
}
```

