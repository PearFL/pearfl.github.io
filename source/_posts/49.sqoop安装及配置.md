---
title: sqoop安装及配置
date: 2020-3-9 00:39:21
tags:
 - sqoop
categories: 大数据
---



## sqoop安装

下载地址：http://archive.cloudera.com/cdh5/cdh/5/sqoop-1.4.6-cdh5.16.2.tar.gz



1.解压

```
tar -zxvf sqoop-1.4.6-cdh5.16.2.tar.gz -C ~/app/
```



2.配置系统环境变量

```
export SQOOP_HOME=/home/hadoop/app/sqoop-1.4.6-cdh5.16.2
export PATH=$SQOOP_HOME/bin:$PATH
```



3.配置conf文件

```
cp sqoop-env-template.sh sqoop-env.sh
export HADOOP_COMMON_HOME/home/pearfl/app/hadoop-2.6.0-cdh5.16.2
export HADOOP_MAPRED_HOME/home/pearfl/app/hadoop-2.6.0-cdh5.16.2
export HIVE_HOME/home/pearfl/app/hive-1.1.0-cdh5.16.2
```



4.导入mysql的jar包

```
cp mysql-connector-java-5.1.47-bin.jar $SQOOP_HOME/lib/
```



5.测试是否与mysql连通

```
sqoop list-databases --connect jdbc:mysql://hadoop000:3306/ --username root --password xxxxxx
```

<!-- more -->

## 导数据模板

### mysql->hdfs

(1).全部导入

```
sqoop import \
--connect jdbc:mysql://hadoop000:3306/database \
--username root \
--password xxxxxx \
--table staff \
--target-dir /user/database \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"
```

(2).查询导入

```
sqoop import \
--connect jdbc:mysql://hadoop000:3306/database \
--username root \
--password xxxxxx \
--table staff \
--target-dir /user/database \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"
--query "select name,sex from table where id <=1 and \$CONDITIONS;"
```

(3).导入指定的列

```
sqoop import \
--connect jdbc:mysql://hadoop000:3306/database \
--username root \
--password xxxxxx \
--table staff \
--colums id,sex
--target-dir /user/database \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"

```

(4).查询条件导入

```
sqoop import \
--connect jdbc:mysql://hadoop000:3306/database \
--username root \
--password xxxxxx \
--table staff \
--colums id,sex
--where "id=1"
--target-dir /user/database \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"
```



### hdfs->mysql

```
sqoop export \
-Dsqoop.export.records.per.statement=10 \
--connect jdbc:mysql://hadoop000:3306/sqoop \
--password xxxxxx \
--username root \
--table emp_demo \
--export-dir /user/hadoop/EMP_COLUMN_SPLIT \
--columns "EMPNO,ENAME,JOB,SAL,COMM" \
--fields-terminated-by '\t' \
-m 1
```



### mysql->Hive

```
sqoop import \
--connect jdbc:mysql://hadoop000:3306/sqoop \
--password xxxxxx \
--username root \
--table emp \
--hive-overwrite \
--delete-target-dir \
--hive-import --hive-database hadoop_hive \
--hive-table emp_import_partition \
--hive-partition-key 'pt' \
--hive-partition-value '2019-12-30' \
--fields-terminated-by '\t' \
-m 1
```



### hive->mysql

```
sqoop export \
--connect jdbc:mysql://hadoop000:3306/sqoop \
--password xxxxxx \
--username root \
--table dept_demo \
--export-dir /user/hive/warehouse/hadoop_hive.db/dept \
--fields-terminated-by '\t' \
-m 1
```

